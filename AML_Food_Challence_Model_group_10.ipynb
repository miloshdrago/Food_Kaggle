{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Food challenge:\n",
    "## A Deep Learning Ensemble-Method for Food Recognition Using Transfer Learning \n",
    "\n",
    "## Group 10\n",
    "Joris van der Vorst, Antonio Javier Samaniego Jurado, Milos Dragojevic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "LD11HOGgL-q5",
    "outputId": "84d94801-9bad-4d3d-d3d4-5d0b63d3765e"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model, model_from_json, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "from sklearn.utils import class_weight\n",
    "import os.path\n",
    "import fnmatch\n",
    "import itertools\n",
    "import functools\n",
    "from math import floor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# Addition Inception Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# Additions for model Antonio\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "GhEnbo7EUyZR",
    "outputId": "769900d6-e090-4505-c430-d938c7ad811d"
   },
   "outputs": [],
   "source": [
    "# Check available model files\n",
    "!mkdir ../working/models\n",
    "!ls ../working\n",
    "!ls ../input/previousfoodmodels/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUw_hLH-ErPF"
   },
   "source": [
    "# General Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Ntx7w3TpEp0i",
    "outputId": "e758aa63-c491-4d2a-b4ba-87e0519b2060"
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "img_width, img_height = 224, 224\n",
    "i3_img_width, i3_img_height = 299, 299\n",
    "data = pd.read_csv('../input/food-recognition-challenge/train_labels.csv',dtype=str)\n",
    "display(data.head())\n",
    "train_data_dir = '../input/food-recognition-challenge/train_set/train_set/'\n",
    "test_data_dir = '../input/food-recognition-challenge/test_set/test_set/'\n",
    "\n",
    "# Model weights\n",
    "checkpoint_vgg= '../working/models/vgg_checkpoint.hdf5'\n",
    "final_vgg= '../working/models/vgg_final.hdf5'\n",
    "json_vgg = \"../working/models/vgg_model.json\"\n",
    "weights_vgg = \"../working/models/vgg_model_weights.h5\"\n",
    "\n",
    "checkpoint_top_i3 = '../working/models/i3_top_checkpoint.hdf5'\n",
    "checkpoint_full_i3 = '../working/models/i3_full_checkpoint.hdf5'\n",
    "final_i3= '../working/models/i3_final.hdf5'\n",
    "json_i3 = \"../working/models/i3_model.json\"\n",
    "weights_i3 = \"../working/models/i3_model_weights.h5\"\n",
    "\n",
    "# Training history data\n",
    "history_vgg= '../working/models/vgg_training.pickle'\n",
    "history_top_i3 = '../working/models/i3_top_training.pickle'\n",
    "history_full_i3 = '../working/models/i3_full_training.pickle'\n",
    "\n",
    "submission_file = \"submission.csv\"\n",
    "\n",
    "epochs_top = 15 #25\n",
    "epochs = 25 #100\n",
    "batch_size = 128\n",
    "patience_num = 3\n",
    "min_delta_ea = 0.0005\n",
    "\n",
    "# Load in model files\n",
    "vgg_model_file = False # \"../input/previousfoodmodels/models/vgg_final.hdf5\"\n",
    "vgg_model_weights = \"../input/previousfoodmodels/models/vgg_checkpoint.hdf5\"\n",
    "vgg_model_json_file = \"../input/previousfoodmodels/models/vgg_model.json\"\n",
    "\n",
    "i3_model_file = False # \"../input/previousfoodmodels/models/i3_final.hdf5\"\n",
    "i3_model_weights = False #\"../input/previousfoodmodels/models/i3_top_checkpoint.hdf5\"\n",
    "i3_model_json_file = False #\"../input/previousfoodmodels/models/i3_model.json\"\n",
    "\n",
    "#Train-test split\n",
    "data_h, holdoutSet = train_test_split(data, test_size=0.1, random_state = 21)\n",
    "#Train-test split\n",
    "trainingSet, validationSet = train_test_split(data_h, test_size=0.2, random_state = 16)\n",
    "\n",
    "\n",
    "# Test flow_from_dataframe\n",
    "sample_csv = pd.read_csv('../input/food-recognition-challenge/sample.csv',dtype=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tkl80xDnF7kr"
   },
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "URivqoFiF_a7",
    "outputId": "877fc647-6ae0-4e34-f001-107c0e1265cf"
   },
   "outputs": [],
   "source": [
    "# Create class weights\n",
    "\n",
    "class_w = class_weight.compute_class_weight('balanced', np.unique(trainingSet[\"label\"]), trainingSet[\"label\"])\n",
    "\n",
    "#VGG Loaders\n",
    "def preprocess_input_vgg(x):\n",
    "    \"\"\"Wrapper around keras.applications.vgg16.preprocess_input()\n",
    "    to make it compatible for use with keras.preprocessing.image.ImageDataGenerator's\n",
    "    `preprocessing_function` argument.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : a numpy 3darray (a single image to be preprocessed)\n",
    "    \n",
    "    Note we cannot pass keras.applications.vgg16.preprocess_input()\n",
    "    directly to to keras.preprocessing.image.ImageDataGenerator's\n",
    "    `preprocessing_function` argument because the former expects a\n",
    "    4D tensor whereas the latter expects a 3D tensor. Hence the\n",
    "    existence of this wrapper.\n",
    "    \n",
    "    Returns a numpy 3darray (the preprocessed image).\n",
    "    \n",
    "    \"\"\"\n",
    "    X = np.expand_dims(x, axis=0)\n",
    "    X = preprocess_input(X)\n",
    "    return X[0]\n",
    "\n",
    "vgg_train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       fill_mode='nearest')\n",
    "\n",
    "vgg_train_generator = vgg_train_datagen.flow_from_dataframe(\n",
    "    dataframe= trainingSet,\n",
    "    x_col=\"img_name\",\n",
    "    y_col=\"label\",\n",
    "    directory=train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    seed = 3)\n",
    "\n",
    "vgg_validation_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n",
    "vgg_validation_generator = vgg_validation_test_datagen.flow_from_dataframe(\n",
    "    dataframe= validationSet,\n",
    "    x_col=\"img_name\",\n",
    "    y_col=\"label\",\n",
    "    directory=train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    seed = 4)\n",
    "\n",
    "vgg_hold_generator = vgg_validation_test_datagen.flow_from_dataframe(\n",
    "    dataframe= holdoutSet,\n",
    "    x_col=\"img_name\",\n",
    "    y_col=\"label\",\n",
    "    directory=train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size)\n",
    "\n",
    "vgg_test_generator = vgg_validation_test_datagen.flow_from_dataframe(\n",
    "    dataframe= sample_csv,\n",
    "    x_col=\"img_name\",\n",
    "    y_col=\"label\",\n",
    "    directory=test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "vgg_train_samples = len(vgg_train_generator.filenames)\n",
    "vgg_validation_samples = len(vgg_validation_generator.filenames)\n",
    "\n",
    "print(\"vgg_Train files: \", len(vgg_train_generator.filenames))\n",
    "print(\"vgg_Validation files: \", len(vgg_validation_generator.filenames))\n",
    "print(\"i3_Holdout files: \", len(vgg_hold_generator.filenames))\n",
    "print(\"vgg_Test files: \", len(vgg_test_generator.filenames))\n",
    "\n",
    "vgg_mapping_indices = vgg_train_generator.class_indices\n",
    "print(vgg_mapping_indices)\n",
    "\n",
    "\n",
    "# Inception V3\n",
    "i3_train_datagen=ImageDataGenerator(rescale=1./255.,\n",
    "                              rotation_range=40,\n",
    "                              width_shift_range=0.2,\n",
    "                              height_shift_range=0.2,\n",
    "                              shear_range=0.2,\n",
    "                              zoom_range=0.2,\n",
    "                              horizontal_flip=True,\n",
    "                              fill_mode='nearest')\n",
    "\n",
    "i3_train_generator=i3_train_datagen.flow_from_dataframe(\n",
    "    dataframe= trainingSet,\n",
    "    directory=train_data_dir,\n",
    "    x_col=\"img_name\",\n",
    "    y_col='label',\n",
    "    batch_size=batch_size,\n",
    "    seed=5,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(i3_img_width, i3_img_height))\n",
    "\n",
    "i3_validation_test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "i3_validation_generator=i3_validation_test_datagen.flow_from_dataframe(\n",
    "    dataframe= validationSet,\n",
    "    directory=train_data_dir,\n",
    "    x_col=\"img_name\",\n",
    "    y_col='label',\n",
    "    batch_size=batch_size,\n",
    "    seed=6,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(i3_img_width, i3_img_height))\n",
    "\n",
    "i3_hold_generator = i3_validation_test_datagen.flow_from_dataframe(\n",
    "    dataframe= holdoutSet,\n",
    "    directory=train_data_dir,\n",
    "    x_col=\"img_name\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(i3_img_width, i3_img_height),\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size)\n",
    "\n",
    "i3_test_generator = i3_validation_test_datagen.flow_from_dataframe(\n",
    "    dataframe= sample_csv,\n",
    "    x_col=\"img_name\",\n",
    "    y_col=\"label\",\n",
    "    directory=test_data_dir,\n",
    "    target_size=(i3_img_width, i3_img_height),\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "i3_mapping_indices = i3_train_generator.class_indices\n",
    "\n",
    "i3_train_samples = len(i3_train_generator.filenames)\n",
    "i3_validation_samples = len(i3_validation_generator.filenames)\n",
    "\n",
    "print(\"i3_Train files: \", len(i3_train_generator.filenames))\n",
    "print(\"i3_Validation files: \", len(i3_validation_generator.filenames))\n",
    "print(\"i3_Holdout files: \", len(i3_hold_generator.filenames))\n",
    "print(\"i3_Test files: \", len(i3_test_generator.filenames))\n",
    "\n",
    "if i3_mapping_indices == vgg_mapping_indices:\n",
    "  print(\"Indices match\")\n",
    "else:\n",
    "  print(\"Indices do not match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1qGcnQF4Gqk"
   },
   "source": [
    "# Model VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "saETRdAV4QzH",
    "outputId": "7054321f-8eee-43b5-eeb4-943e00d55ac7"
   },
   "outputs": [],
   "source": [
    "#First check is there is a final model\n",
    "if os.path.isfile(vgg_model_file):\n",
    "    # Load in final model\n",
    "    vgg_model = load_model(vgg_model_file)\n",
    "# Otherwise check if there are checkpoint weights\n",
    "elif os.path.isfile(vgg_model_weights):\n",
    "    # If there are weights, see if there is a json file with the structure of the model\n",
    "    if os.path.isfile(vgg_model_json_file):\n",
    "        # Load in model structure and weights\n",
    "        with open(vgg_model_json_file,'r') as f:\n",
    "            vgg_model = model_from_json(f.read())\n",
    "        vgg_model.load_weights(vgg_model_weights)\n",
    "else:\n",
    "\n",
    "    vgg16 = VGG16(weights='imagenet', include_top = True)\n",
    "    vgg16.summary()\n",
    "    x  = vgg16.get_layer('fc2').output\n",
    "    # x = Flatten(name='flatten2')(x)\n",
    "    x = Dense(4096, activation='relu', name='d1')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(4096, activation='relu', name='d2')(x)\n",
    "    prediction = Dense(80, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    vgg_model = Model(inputs=vgg16.input, outputs=prediction)\n",
    "    vgg_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# Freeze All Layers Except Bottleneck Layers for Fine-Tuning\n",
    "\n",
    "for layer in vgg_model.layers:\n",
    "    if layer.name in ['predictions', 'd1', 'd2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3']:\n",
    "        continue\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "df = pd.DataFrame(([layer.name, layer.trainable] \n",
    "                  for layer in vgg_model.layers), columns=['layer', 'trainable'])\n",
    "\n",
    "sgd = SGD(lr=1e-4, momentum=0.9, clipvalue = 0.5)\n",
    "\n",
    "vgg_model.compile(optimizer=sgd, \n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "plot_model(vgg_model, to_file='vgg_model.png',show_shapes=True, \n",
    "           show_layer_names=True, dpi = 900)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = vgg_model.to_json()\n",
    "with open(json_vgg, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZew2B5YlGi7"
   },
   "source": [
    "# Inception V3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VWQ3gIWpjs41"
   },
   "outputs": [],
   "source": [
    "# Inception V3 model\n",
    "base_i3_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "#First check is there is a final model\n",
    "if os.path.isfile(i3_model_file):\n",
    "    # Load in final model\n",
    "    i3_model = load_model(i3_model_file)\n",
    "# Otherwise check if there are checkpoint weights\n",
    "elif os.path.isfile(i3_model_weights):\n",
    "    # If there are weights, see if there is a json file with the structure of the model\n",
    "    if os.path.isfile(i3_model_json_file):\n",
    "        # Load in model structure and weights\n",
    "        with open(i3_model_json_file,'r') as f:\n",
    "            i3_model = model_from_json(f.read())\n",
    "        i3_model.load_weights(i3_model_weights)\n",
    "else:\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_i3_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    # let's add a fully-connected layer (reduced from 1024 to 124)\n",
    "    x = Dense(124, activation='relu')(x)\n",
    "    # and a logistic layer, we have 80 classes\n",
    "    predictions = Dense(80, activation='softmax')(x)\n",
    "\n",
    "    i3_model = Model(inputs=base_i3_model.input, outputs=predictions)\n",
    "    \n",
    "    if i3_model_json_file:\n",
    "        with open(i3_model_json_file,'r') as f:\n",
    "            i3_model = model_from_json(f.read())\n",
    "        i3_model.load_weights(vgg_model_file)\n",
    "\n",
    "for layer in base_i3_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "i3_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "plot_model(i3_model, to_file='i3_model.png',show_shapes=True, \n",
    "           show_layer_names=True, dpi = 900)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = i3_model.to_json()\n",
    "with open(json_i3, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsKu4mYcgVP7"
   },
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models should stop if no improvement on validation for a number of epochs\n",
    "es = EarlyStopping(monitor='val_accuracy', patience= patience_num,\n",
    "                   verbose=1, min_delta = min_delta_ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pMGT4c7P4qsa",
    "outputId": "d8bdbb66-e17c-41f5-9b9f-f11e03de09b1"
   },
   "outputs": [],
   "source": [
    "# Train Model VGG\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_vgg = ModelCheckpoint(checkpoint_vgg, monitor='val_accuracy', verbose=1, \n",
    "                         save_best_only=True, \n",
    "                         save_weights_only=False, mode='auto')\n",
    "\n",
    "vgg_history = vgg_model.fit_generator(\n",
    "        vgg_train_generator,\n",
    "        epochs= epochs,\n",
    "        validation_data= vgg_validation_generator,\n",
    "        steps_per_epoch= vgg_train_samples // batch_size,\n",
    "        validation_steps= vgg_validation_samples // batch_size,\n",
    "        class_weight=class_w,\n",
    "        callbacks=[mc_vgg, es])\n",
    "\n",
    "# serialize weights to HDF5\n",
    "vgg_model.save_weights(weights_vgg)\n",
    "\n",
    "vgg_model.save(final_vgg)\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# Save training data:\n",
    "with open(history_vgg, 'w') as f:\n",
    "    pickle.dump(vgg_history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHLsHx0kkbQP"
   },
   "outputs": [],
   "source": [
    "# # Train InceptionV3 top\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_i3_top = ModelCheckpoint(checkpoint_top_i3, monitor='val_accuracy', verbose=1, \n",
    "                            save_best_only=True, \n",
    "                            save_weights_only=False, mode='auto')\n",
    "\n",
    "i3_top_history = i3_model.fit_generator(\n",
    "        i3_train_generator,\n",
    "        epochs=  epochs_top,\n",
    "        validation_data= i3_validation_generator,\n",
    "        steps_per_epoch=i3_train_samples // batch_size,\n",
    "        validation_steps=i3_validation_samples // batch_size,\n",
    "        class_weight=class_w,\n",
    "        callbacks=[mc_i3_top, es])\n",
    "\n",
    "# serialize weights to HDF5\n",
    "i3_model.save_weights(weights_i3)\n",
    "\n",
    "# Save training data:\n",
    "with open(history_top_i3, 'wb') as f:\n",
    "    pickle.dump(i3_top_history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVoLDBwml7Ht"
   },
   "outputs": [],
   "source": [
    "# Train InceptionV3 full\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_i3_full = ModelCheckpoint(checkpoint_full_i3, monitor='val_accuracy', verbose=1,\n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False, mode='auto')\n",
    "\n",
    "# Open up layers\n",
    "for layer in i3_model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in i3_model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "i3_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), \n",
    "                 loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "i3_full_history = i3_model.fit_generator(\n",
    "        i3_train_generator,\n",
    "        epochs= epochs,\n",
    "        validation_data= i3_validation_generator,\n",
    "        steps_per_epoch=i3_train_samples // batch_size,\n",
    "        validation_steps=i3_validation_samples // batch_size,\n",
    "        class_weight=class_w,\n",
    "        callbacks=[mc_i3_full, es])\n",
    "\n",
    "\n",
    "# serialize weights to HDF5\n",
    "i3_model.save_weights(weights_i3)\n",
    "\n",
    "i3_model.save(final_i3)\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# Save training data:\n",
    "with open(history_full_i3, 'wb') as f:\n",
    "    pickle.dump(i3_full_history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ace0UTk45OXt"
   },
   "outputs": [],
   "source": [
    "# Plots VGG16\n",
    "# Summarize history for accuracy\n",
    "plt.plot(vgg_history.history['accuracy'])\n",
    "plt.plot(vgg_history.history['val_accuracy'])\n",
    "plt.title('VGG16 model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vgg_acc_1.eps', format='eps', dpi=900)\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(vgg_history.history['loss'])\n",
    "plt.plot(vgg_history.history['val_loss'])\n",
    "plt.title('VGG16 model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vgg_val_1.eps', format='eps', dpi=900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pKKKIp0XbaT"
   },
   "outputs": [],
   "source": [
    "# Plots Inception V3 top\n",
    "# Summarize history for accuracy\n",
    "plt.plot(i3_top_history.history['accuracy'])\n",
    "plt.plot(i3_top_history.history['val_accuracy'])\n",
    "plt.title('Inception V3 top model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('i3_top_acc_1.eps', format='eps', dpi=900)\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(i3_top_history.history['loss'])\n",
    "plt.plot(i3_top_history.history['val_loss'])\n",
    "plt.title('Inception V3 top model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('i3_top_val_1.eps', format='eps', dpi=900)\n",
    "plt.show()\n",
    "\n",
    "# Plots I3 full\n",
    "# Summarize history for accuracy\n",
    "plt.plot(i3_full_history.history['accuracy'])\n",
    "plt.plot(i3_full_history.history['val_accuracy'])\n",
    "plt.title('Inception V3 full model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('i3_acc_1.eps', format='eps', dpi=900)\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(i3_full_history.history['loss'])\n",
    "plt.plot(i3_full_history.history['val_loss'])\n",
    "plt.title('Inception V3 full model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('i3_val_1.eps', format='eps', dpi=900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "saLs9sFQSIaz"
   },
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best validation checkpoints for predictions:\n",
    "if os.path.isfile(checkpoint_vgg):\n",
    "    vgg_model.load_weights(checkpoint_vgg)\n",
    "if os.path.isfile(checkpoint_full_i3):\n",
    "    i3_model.load_weights(checkpoint_full_i3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ut13N_wSSLYt"
   },
   "outputs": [],
   "source": [
    "vgg_evaluate = vgg_model.evaluate_generator(vgg_hold_generator,verbose=1)\n",
    "print(vgg_evaluate)\n",
    "print(vgg_model.metrics_names)\n",
    "\n",
    "i3_evaluate = i3_model.evaluate_generator(i3_hold_generator,verbose=1)\n",
    "print(i3_evaluate)\n",
    "\n",
    "total_performance = vgg_evaluate[1] + i3_evaluate[1]\n",
    "print(total_performance)\n",
    "weigth_vgg = vgg_evaluate[1]/total_performance\n",
    "weigth_i3 = i3_evaluate[1]/total_performance\n",
    "\n",
    "print(f\"\"\"Weight VGG Model: {weigth_vgg}\\nWeight Inception V3 Model: {weigth_i3}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weigth_vgg + weigth_i3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNtZw4A3zdal"
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "ue5zCphKDltN",
    "outputId": "b3512dac-99a1-4b65-c4f7-472a8161347c"
   },
   "outputs": [],
   "source": [
    "# VGG\n",
    "# Reset to ensure files are loaded in the correct order\n",
    "vgg_test_generator.reset()\n",
    "# Genarate predictions\n",
    "vgg_predict = vgg_model.predict_generator(vgg_test_generator, verbose = 1)\n",
    "\n",
    "# Test if amount of predictions match the number of examples\n",
    "print(\"Amount of predictions match the number of examples: \",\n",
    "      len(vgg_predict) == sample_csv.shape[0])\n",
    "\n",
    "# Inception V3\n",
    "# Reset to ensure files are loaded in the correct order\n",
    "i3_test_generator.reset()\n",
    "# Genarate predictions\n",
    "i3_predict = i3_model.predict_generator(i3_test_generator, verbose = 1)\n",
    "\n",
    "\n",
    "# Test if amount of predictions match the number of examples\n",
    "print(\"Amount of predictions match the number of examples: \",\n",
    "      len(vgg_predict) ==len(i3_predict)  == sample_csv.shape[0])\n",
    "\n",
    "print(\"All indices match: \",\n",
    "      vgg_mapping_indices == i3_mapping_indices)\n",
    "\n",
    "print(\"All filenames match: \",\n",
    "      vgg_test_generator.filenames == i3_test_generator.filenames)\n",
    "\n",
    "# Match highest scoring column index to label\n",
    "labels = dict((v,k) for k,v in vgg_mapping_indices.items())\n",
    "# Add labels of both models together\n",
    "total_predict = (vgg_predict*weigth_vgg  + i3_predict*weigth_i3)\n",
    "\n",
    "print(total_predict.shape)\n",
    "predictions = [labels[k] for k in np.argmax(total_predict,axis=1)]\n",
    "\n",
    "prediction = pd.DataFrame({\"img_name\": vgg_test_generator.filenames, \"label\":predictions})\n",
    "display(prediction.head())\n",
    "# Save submission as csv\n",
    "prediction.to_csv(submission_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Food_Challenge_Bagged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
